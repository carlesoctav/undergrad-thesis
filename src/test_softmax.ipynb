{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BaseModel import SentenceEmbedder, MeanPooling, Normalize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from losses.Softmax import SoftmaxTrainer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carles/undergrad-thesis/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'pooling_layer' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['pooling_layer'])`.\n",
      "  rank_zero_warn(\n",
      "/home/carles/undergrad-thesis/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'normalize_layer' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['normalize_layer'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model_1 = SoftmaxTrainer(\n",
    "    pretrained_model= \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    pooling_layer= MeanPooling(),\n",
    "    normalize_layer= Normalize(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test take input\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sentence_1 = [\"I like to eat apples.\"]\n",
    "input = tokenizer(sentence_1, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2066,  2000,  4521, 18108,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0287e-02, -1.0149e-02,  4.4379e-03,  9.8936e-02, -7.5347e-02,\n",
       "         -3.1618e-02,  1.2500e-01, -7.0418e-02,  3.1395e-02,  1.3503e-02,\n",
       "          4.7869e-02, -1.1311e-01,  2.2448e-02,  4.3147e-03,  4.1479e-02,\n",
       "         -3.1775e-02,  6.4389e-02, -3.9002e-03, -2.0846e-02,  2.3466e-02,\n",
       "         -7.8653e-02,  6.1797e-02,  5.7146e-02, -2.2043e-02, -8.3824e-03,\n",
       "          5.8999e-02,  5.5665e-02, -3.1469e-02, -9.1467e-02, -1.2344e-02,\n",
       "         -6.5881e-02, -3.4765e-03,  1.1020e-02,  2.4454e-02, -3.5012e-02,\n",
       "         -6.7719e-03,  1.3620e-01, -7.4847e-02, -3.0549e-02, -6.1541e-03,\n",
       "          3.0408e-02,  5.5383e-02,  5.7821e-02,  3.7068e-02, -7.2129e-02,\n",
       "         -7.6281e-03,  7.6650e-03, -1.8241e-02,  1.0269e-01,  4.7048e-02,\n",
       "          3.0462e-02, -2.0631e-03, -3.7888e-02, -4.1398e-02,  5.3176e-02,\n",
       "          5.4241e-02,  1.4381e-02,  1.1565e-02,  1.3236e-02,  1.0639e-02,\n",
       "          6.3619e-02, -6.9950e-02, -1.4093e-03,  2.6537e-02,  3.2536e-02,\n",
       "         -6.5808e-02, -6.1245e-02,  1.3085e-02, -6.8123e-02, -1.1782e-02,\n",
       "         -1.2302e-02,  3.8724e-02,  7.1766e-02,  4.6170e-02, -1.9104e-02,\n",
       "          2.3216e-02,  1.0896e-01, -8.1457e-02, -5.4294e-02,  8.9858e-03,\n",
       "         -4.4968e-02, -4.4236e-03,  1.3730e-02, -7.5823e-04, -3.4269e-02,\n",
       "         -2.6818e-02, -1.4833e-02,  1.4883e-02, -4.2866e-02,  7.7979e-02,\n",
       "         -1.0276e-02,  1.5357e-03, -5.4119e-02,  6.5657e-02,  9.9451e-03,\n",
       "          2.0147e-02,  2.9057e-02, -7.4371e-02, -2.3502e-02,  8.2725e-02,\n",
       "          3.5503e-02,  1.9429e-02,  1.9942e-04,  5.7564e-02,  1.6644e-02,\n",
       "         -5.1608e-03, -1.5894e-01,  3.0221e-03,  7.2424e-04, -8.5799e-03,\n",
       "          1.9068e-02, -3.3247e-02, -3.1412e-02, -1.0916e-02,  1.7319e-02,\n",
       "          3.2999e-02,  1.6428e-02,  1.0450e-02,  2.6644e-02, -1.1674e-02,\n",
       "          1.6262e-02,  1.1560e-01, -8.7963e-02, -5.8497e-03, -3.0242e-03,\n",
       "         -4.0210e-02,  1.1695e-02, -6.8604e-33, -9.5377e-02, -4.5260e-02,\n",
       "          4.6649e-02, -2.5693e-02, -5.6303e-03,  5.5314e-03,  4.0174e-02,\n",
       "          9.6319e-02,  7.4676e-02, -6.9565e-03, -1.9796e-02, -1.4552e-02,\n",
       "         -4.9932e-02,  1.0697e-02,  1.5486e-02, -3.9437e-02, -1.8491e-02,\n",
       "          6.4598e-02, -4.5725e-02, -1.3239e-02,  2.2600e-03, -9.4809e-02,\n",
       "         -1.2989e-02,  7.8608e-02, -2.7659e-02, -7.9498e-02,  3.4714e-02,\n",
       "         -1.3114e-01,  7.9143e-02,  1.6519e-02,  2.9319e-02, -4.6572e-03,\n",
       "         -3.6750e-02, -7.8688e-02, -5.3226e-03, -1.7486e-02,  7.4181e-02,\n",
       "          4.2305e-02, -3.2836e-02,  1.9365e-02, -1.3959e-03,  1.1381e-02,\n",
       "          7.3268e-02,  3.7600e-02,  6.9289e-02,  9.1143e-04,  2.8295e-02,\n",
       "          6.6940e-02, -2.3906e-02, -1.1573e-02, -5.7530e-02, -5.0233e-02,\n",
       "          7.5773e-02,  2.3939e-02, -7.0374e-03, -5.2323e-03,  1.5755e-02,\n",
       "         -4.3599e-02, -8.5273e-02,  2.6363e-02, -7.0925e-02,  3.3230e-02,\n",
       "          4.2529e-03,  9.5256e-03, -1.2457e-01,  1.0991e-01, -1.8965e-02,\n",
       "         -4.5380e-02,  2.2038e-02,  7.7254e-03, -4.9068e-02,  2.7177e-02,\n",
       "         -2.6616e-02,  2.5021e-02, -1.0382e-01, -8.8923e-02,  3.5611e-02,\n",
       "         -2.7225e-02, -4.5574e-02, -4.4567e-02,  6.2558e-02,  3.1060e-03,\n",
       "          8.7305e-03,  7.7758e-04,  7.0589e-02,  1.1827e-01, -6.4955e-02,\n",
       "         -9.4032e-02,  1.0448e-01, -8.8006e-03,  1.1977e-02,  2.8457e-02,\n",
       "         -8.4088e-03,  1.7963e-03, -1.1155e-01,  5.3809e-33, -7.6003e-03,\n",
       "         -8.2957e-02, -3.7390e-02,  1.0053e-03, -2.8514e-02, -4.5829e-02,\n",
       "         -7.2399e-02,  3.7799e-03, -1.8175e-02, -3.1740e-02, -3.1873e-02,\n",
       "         -3.2478e-03, -2.2166e-03, -1.8439e-02,  1.9787e-02,  1.9432e-02,\n",
       "         -4.8507e-03,  9.2745e-02, -2.8534e-02,  1.4055e-02, -5.1488e-02,\n",
       "         -1.5350e-02, -1.0200e-02,  2.2659e-02,  4.9719e-03, -3.1963e-02,\n",
       "         -4.5188e-03,  3.7165e-02, -3.2132e-02,  6.0620e-02,  2.6282e-02,\n",
       "         -9.1577e-02, -7.8656e-02, -1.1859e-01,  4.7620e-02,  4.1995e-02,\n",
       "         -7.6050e-02, -4.1291e-02, -3.4322e-03,  5.0894e-02,  1.5019e-02,\n",
       "         -1.0739e-02,  3.2360e-02,  1.1745e-01,  3.1321e-02,  9.9526e-02,\n",
       "          2.8065e-02,  6.4154e-02,  5.1111e-02,  3.4007e-02,  9.0730e-03,\n",
       "         -2.1948e-02, -6.0511e-02,  4.9400e-03,  2.9833e-02,  5.4973e-02,\n",
       "          5.5136e-03,  3.2863e-03, -1.9241e-03, -6.7784e-02, -1.0324e-01,\n",
       "          5.4861e-02,  1.1183e-02, -1.1372e-03,  5.6880e-02, -1.4619e-02,\n",
       "         -2.4292e-02, -5.2211e-02, -8.8502e-02,  1.3864e-02, -5.4098e-03,\n",
       "         -5.1055e-02, -4.9262e-02, -9.2913e-03, -5.0746e-02, -5.5569e-02,\n",
       "          2.2190e-02,  2.3268e-03, -9.6305e-02,  4.0000e-02, -3.0955e-02,\n",
       "          1.0340e-01,  3.0380e-02,  8.2878e-02,  7.0387e-02,  4.9900e-02,\n",
       "         -1.9792e-02,  3.8635e-02, -1.4044e-02,  2.1018e-02, -7.4588e-03,\n",
       "         -8.1476e-03,  1.2419e-02, -3.8648e-02, -1.8072e-02, -1.6188e-08,\n",
       "         -4.0603e-02, -1.7830e-02,  2.2321e-02, -1.3404e-02, -1.0267e-02,\n",
       "          8.1561e-02, -8.6338e-02,  3.5637e-02,  1.4574e-02, -5.1561e-02,\n",
       "          5.1365e-02,  8.5692e-02, -7.3924e-02,  5.4521e-02,  6.8129e-02,\n",
       "         -6.8178e-03,  7.6340e-02, -3.2330e-02, -5.1194e-03,  6.3627e-02,\n",
       "         -8.9008e-02,  5.4309e-03, -2.1059e-02,  4.2669e-02, -2.0280e-02,\n",
       "          1.6460e-02,  1.8217e-02,  2.9360e-02,  1.2294e-01,  7.7443e-02,\n",
       "          5.1649e-02,  3.4369e-02, -9.0554e-02,  4.1706e-02, -2.5796e-02,\n",
       "         -9.0973e-02,  6.9601e-04, -1.1604e-02, -1.3693e-02, -9.4786e-02,\n",
       "         -5.0627e-02, -6.9923e-03,  3.0214e-02, -2.8363e-02, -1.0387e-01,\n",
       "          3.9577e-02,  5.5180e-02,  5.9584e-04,  4.0414e-02,  9.5169e-02,\n",
       "          1.6818e-02,  3.1306e-03,  1.1280e-01,  6.4993e-02,  1.2183e-02,\n",
       "         -2.1353e-02,  2.1665e-02, -8.4117e-02,  2.0306e-02,  8.2033e-02,\n",
       "          1.2463e-01,  6.3424e-02,  1.4999e-02, -1.5381e-02]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_1(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/undergrad-thesis/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/undergrad-thesis/src/losses/Softmax.py:25\u001b[0m, in \u001b[0;36mSoftmaxTrainer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/undergrad-thesis/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "model_1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/carles/.cache/huggingface/datasets/carlesoctav___parquet/carlesoctav--en-id-parallel-sentences-2ac6d941a9b892f7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d94c148a58f41c8901c52e6acbd1d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"carlesoctav/en-id-parallel-sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataset\n",
    "dataset = dataset[\"QED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=['text_en', 'text_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091d67fa599445d6a5dafac81f3b8e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/274581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"text_en\"], x[\"text_id\"], padding=\"max_length\", truncation=True, max_length=512), batched=True,num_proc=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data = DataLoader(dataset, batch_size=32, num_workers=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_en': ['In the time of Papuan independence, the Papuan flag and the Dutch flag raised together The transition took place when UN came and the Dutch left Papua. The Dutch flag was brought down, only the Papuan and UN flag were flying', 'So we knew that the Ducth was hesitant, the dutch wanted to grant the independence for the people in Papua. But the Papua people cannot acknowledge this secret. Then the Dutch is forced to give the Independence to the Papuan', \"The Dutch gave the Independence to Papuan but that was not aired on Radio Republic of Indonesia that is why i have been given a mandate to read the PEPERA text that is crucial in determining Papuan's destiny in a month i was trained to read day and night as I read the PEPERA text there were no mistake allowed because the text is handwriting on 14th July 1969, i had to read it\", 'The illeterate people in Papua were only given the stamp by the leader of the village', 'we have accutomed to rule the country with the tradition that has existed since the day we were born.', 'Our anchestors could make it, we are big nation, we should not let the small island rule us.', \"At that time, they had seen many Papuan people in PEPERA incident. The Army were so strict so the people were afraid of doing anything. There were a lot of soldiers patrolling the streets, and the people were scared to step out of house they did not do anything what they supposed to do, and the very first thing i heard, Indonesia finally took over the Papua Island no matter the Papuan's desire, the most important was we took over the Island that's the main goal because in my point of view PEPERA was illegal and broken.\", 'PEPERA agreement was decided but the older still suffered For papuan there was no peace before all the native people were terminated And when the troubles were solved, God created this island', 'becaus this problem discussed about us, we can have demonstration but we cannot brng bring our attribute through in PEPERA was terminated and it was supported by England so Papua is free', 'for instance the people were killed and kidnapped by the violence during the PEPERA days there were significant differences between the Dutch and Indonesia government, and we were very traumatic because of the military', 'Companies are losing control.', 'What happens on Wall Street no longer stays on Wall Street.', 'What happens in Vegas ends up on YouTube.', '(Laughter)', 'Reputations are volatile.', 'Loyalties are fickle.', 'Management teams seem increasingly disconnected from their staff. (Laughter)', 'A recent survey said that 27 percent of bosses believe their employees are inspired by their firm.', 'However, in the same survey, only four percent of employees agreed.', 'Companies are losing control of their customers and their employees.', 'But are they really?', \"I'm a marketer, and as a marketer, I know that I've never really been in control.\", \"Your brand is what other people say about you when you're not in the room, the saying goes.\", 'Hyperconnectivity and transparency allow companies to be in that room now, 24/7.', 'They can listen and join the conversation.', 'In fact, they have more control over the loss of control than ever before.', 'They can design for it.', 'But how?', 'First of all, they can give employees and customers more control.', 'They can collaborate with them on the creation of ideas, knowledge, content, designs and product.', 'They can give them more control over pricing, which is what the band Radiohead did with its pay-as-you-like online release of its album \\\\\"In Rainbows.\\\\\" Buyers could determine the price, but the offer was exclusive, and only stood for a limited period of time.', 'The album sold more copies than previous releases of the band.'], 'text_id': ['waktu kita merdeka bendera Papua dan bendera Papua sama sama berkibar, datang peralihan itu PBB masuk disini dan Belanda keluar dari Papua, bendera Belanda diturunkan dan hanya bendera PBB dan Papua yang berkibar datang indonesia penguasa Indonesia masuk, baru bendera Papua turun sampai sekarang', 'jadi kita tahu belanda ragu ragu, Belanda ingin berikan kemerdekaan ke rakyat Papua tetapi rahasia ini tidak boleh orang Papua tahu jadi dengan keadaan terpaksa Belanda memberikan kemerdekaan bagi Papua', 'Belanda sudah berikan kemerdekaan ke Papua tetapi tidak diberitakan melalui radio RRI jadi saya harus membaca teks PEPERA yang menentukan nasib Papua ini saya diberikan amanat itu untuk baca jadi selama satu bulan itu saya dilatih terus untuk membaca siang dan malam saya sewaktu membaca teks Pepera tidak boleh satu kata yang salah, karena tulisan itu bukan di ketik tapi ditulis tangan pada saat 14 Juli 1969 saya harus membacanya', 'orang orang kampung yang tidak bisa membaca hanya diberikan cap saja oleh kepala kampung', 'tidak bisa kita seperti ini terus, kita atur negeri ini dengan adat istiadat yang sudah diatur begitu rupa dari saat kita dilahirkan', 'leluhur dulu bisa, untuk apa pulau kecil saja mengatur kita, kita ini pulau besar', 'kejadian waktu itu banyak orang Papua yang mereka sudah liat sewaktu Pepera tentara juga ketat jadi masyarakat mau berbuat apa apa sulit, takut yah banyak tentara yang turun, karena takut masyarakat tidak berani keluar rumah tidak berbuat sesuatu yang mereka inginkan, pertama saya dengar, indonesia ambil Papua bukan karena rakyatnya, rakyat menjadi apa saja tidak penting, yang penting kita rebut tanah itu tujuan utama, kalau menurut saya Pepera itu tidak benar dan cacat hukum', 'Pepera sudah selesai tetapi orang orang tua yang hidup masih menderita untuk Papua tidak ada damai, kecuali orang yang berambut keriting ini habis baru persoalan Papua selesai,Tuhan menciptakan negeri ini dengan alamnya kita punya, kenapa orang lain yang mau mengatur kami, banyak orang yang jadi korban kalau saya sudah besar saya akan minta kembali saya punya hak anak Papua yang ada ditempat ini, anak Papua juga punya hak untuk mendukung', 'kegiatan ini karena membahas persoalan kami, boleh demo tetapi tidak boleh membawa atribut Papua didukung Inggris mengenai Pepera di Papua untuk dicabut, bagaimana Papua ini terlepas', 'kejahatan terhadap masyarakat ini bisa diculik atau dibunuh, itu yang jadi perbedaan antara pemerintah Belanda dan Indonesia, kami trauma karena perlakuan oknum militer', 'Perusahaan-perusahaan kini telah hilang kendali.', 'Berita tentang apa yang terjadi di Wall Street tidak lagi berhenti di Wall Street.', 'Apa yang terjadi di Las Vegas akan terlihat di YouTube.', '(tawa)', 'Reputasi tidak lagi stabil.', 'Loyalitas mudah berubah.', 'Tim manajemen menjadi lebih terlepas hubungannya dari staff.', 'Survei mengatakan 27 persen dari para bos percaya bahwa pegawai mereka terinspirasi oleh perusahaannya.', 'Namun, dalam survei yang sama, hanya empat persen dari pegawai tersebut menyetujuinya.', 'Perusahaan-perusahaan kian hilang kendali akan pelanggan dan pegawainya.', 'Tapi benarkah ini?', 'Saya seorang tenaga pemasar dan saya tahu baik bahwa saya jarang sekali memiliki kendali.', 'Kata pepatah, merek Anda adalah apa yang orang lain katakan tentang Anda saat Anda tidak ada di dalam ruangan.', 'Hyper-konektivitas dan transparansi memungkinkan perusahaan tersebut untuk terus berada di dalam ruangan tersebut, sepanjang waktu.', 'Mereka dapat mendengar dan berdiskusi bersama.', 'Bahkan, kini mereka lebih mempunyai kendali terhadap kehilangan kendali, lebih dari sebelumnya.', 'Mereka dapat berencana untuk itu.', 'Tapi bagaimana?', 'Pertama, mereka dapat menyerahkan kendali kepada pegawai dan pelanggan.', 'Mereka dapat berkolaborasi dengannya dalam penyusunan ide, pengetahuan, konten, desain dan produk.', 'Perusahaan dapat memberikan kendali tentang penentuan harga kepada karyawan &amp; pelanggan, seperti yang dilakukan oleh band Radiohead dengan sistem bayar-sesuka-Anda saat merilis album mereka yang berjudul \\\\\"In Rainbows.\\\\\" Pembeli dapat menentukan harga, tetapi tawarannya ekslusif, dan hanya berlaku untuk jangka waktu pendek.', 'Jumlah album yang terjual melebihi rilis-rilis mereka yang sebelumnya.'], 'input_ids': tensor([[ 101, 1999, 1996,  ...,    0,    0,    0],\n",
      "        [ 101, 2061, 2057,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 3803,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2027, 2064,  ...,    0,    0,    0],\n",
      "        [ 101, 2027, 2064,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 2201,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in data:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
